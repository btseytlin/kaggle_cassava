{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nbformat as nbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-09 19:31:35,829 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...\n"
     ]
    }
   ],
   "source": [
    "parameters = context.catalog.load('parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_notebook(cells, name):\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    nb['cells'] = cells\n",
    "    nbf.write(nb, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imports(text):\n",
    "    imports = []\n",
    "    for line in text.split('\\n'):\n",
    "        if (line.startswith('import') \n",
    "            or line.startswith('from') \n",
    "            and (not 'from .' in line)\n",
    "            and not ('kedro' in line)\n",
    "            and not 'cassava' in line):\n",
    "            imports.append(line.strip())\n",
    "    return imports\n",
    "\n",
    "def get_imports():\n",
    "    imports = []\n",
    "    path = '../src/cassava/pipelines'\n",
    "    for dirname in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, dirname)) or dirname.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(os.path.join(path, dirname)):\n",
    "            if not fname.endswith('.py'):\n",
    "                continue\n",
    "            fpath = os.path.join(path, dirname, fname)\n",
    "            with open(fpath) as f:\n",
    "                file_imports = extract_imports(f.read())\n",
    "                imports += file_imports\n",
    "    imports = list(set(imports))\n",
    "    return '\\n'.join(imports)\n",
    "            \n",
    "            \n",
    "def get_helper_files_cells():\n",
    "    cells = []\n",
    "    \n",
    "    path = '../src/cassava/'\n",
    "    target_files = ['transforms.py', 'utils.py', 'models/model.py', 'node_helpers.py']\n",
    "    \n",
    "    for fname in target_files:\n",
    "        fpath = os.path.join(path, fname)\n",
    "        \n",
    "        with open(fpath) as f:\n",
    "            cells.append(nbf.v4.new_code_cell(f'# file {fname}\\n\\n{f.read()}'))\n",
    "    return cells\n",
    "\n",
    "    \n",
    "def get_pipeline_def_cell(pipeline, name):\n",
    "    funcs = [\n",
    "        f\"#Pipeline {name}\"\n",
    "    ]\n",
    "    for node in pipeline.nodes:\n",
    "        func_source_node = inspect.getsource(node._func)\n",
    "        funcs.append(func_source_node)\n",
    "        \n",
    "    cell_code = \"\\n\\n\".join(funcs)\n",
    "    return nbf.v4.new_code_cell(cell_code)\n",
    "\n",
    "def get_pipeline_execution_cells(pipeline):\n",
    "    cells = []\n",
    "    nodesets = pipeline._topo_sorted_nodes\n",
    "    for nodeset in nodesets:\n",
    "        while nodeset:\n",
    "            node = nodeset.pop()\n",
    "            if node.outputs:\n",
    "                node_code = f\"\"\"{\", \".join(node.outputs)} = {node._func_name}({\", \".join(node.inputs)})\"\"\"\n",
    "            else:\n",
    "                node_code = f\"\"\"{node._func_name}({\", \".join(node.inputs)})\"\"\"\n",
    "            \n",
    "            cells.append(nbf.v4.new_code_cell(node_code))\n",
    "    return cells\n",
    "        \n",
    "    \n",
    "def get_notebook_cells(parameters, pipelines, initial_cells, extra_cells, final_cell):\n",
    "    imports_cell = nbf.v4.new_code_cell(get_imports())\n",
    "    \n",
    "    helper_files_cells = get_helper_files_cells()\n",
    "    parameters_cell = nbf.v4.new_code_cell(f\"\"\"parameters = {json.dumps(parameters, indent=4, default=str)}\"\"\")\n",
    "    \n",
    "    pipeline_def_cells = []\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name.startswith('_'):\n",
    "            continue\n",
    "        pipeline_def_cells.append(get_pipeline_def_cell(pipeline, name))\n",
    "    \n",
    "    pipeline_exec_cells = []\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if not name in ['cv', 'predict']:\n",
    "            continue\n",
    "        pipeline_exec_cells += get_pipeline_execution_cells(pipeline)\n",
    "    return [imports_cell,\n",
    "            *initial_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Functions\"),\n",
    "            *helper_files_cells,\n",
    "            *pipeline_def_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Parameters\"),\n",
    "            parameters_cell, \n",
    "            *extra_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Execution\"),\n",
    "           *pipeline_exec_cells,\n",
    "           final_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cell = nbf.v4.new_code_cell('%matplotlib inline')\n",
    "\n",
    "data_cell = nbf.v4.new_code_cell(\"\"\"\n",
    "DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'\n",
    "\n",
    "train_labels = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "sample_submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "label_num_to_disease_map = pd.read_csv(f'{DATA_DIR}/label_num_to_disease_map.json')\n",
    "\n",
    "train_images_torch = CassavaDataset(image_ids=train_labels.image_id.values, labels=train_labels.label.values, root=f'{DATA_DIR}/train_images')\n",
    "test_images_torch = CassavaDataset(image_ids=sample_submission.image_id.values, labels=sample_submission.label.values, root=f'{DATA_DIR}/test_images')\n",
    "\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "\"\"\")\n",
    "\n",
    "models_cell = nbf.v4.new_code_cell(\"\"\"\n",
    "import sys\n",
    "\n",
    "efficientnet_path='/kaggle/input/efficientnet-pytorch'\n",
    "\n",
    "sys.path.append(efficientnet_path)\n",
    "\n",
    "!ls /kaggle/input/timm-pretrained-efficientnet\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/timm-pretrained-efficientnet/efficientnet/efficientnet_b0_ra-3dd342df.pth /root/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth\n",
    "\n",
    "!pip install /kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl\n",
    "\"\"\")\n",
    "\n",
    "final_cell = nbf.v4.new_code_cell(\"\"\"\n",
    "print(cv_results['summary'])\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_notebook(\n",
    "    get_notebook_cells(parameters, \n",
    "                                   context.pipelines, \n",
    "                                   initial_cells=[models_cell, initial_cell],\n",
    "                                   extra_cells=[data_cell], \n",
    "                                   final_cell=final_cell), \n",
    "    'submission.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cassava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
