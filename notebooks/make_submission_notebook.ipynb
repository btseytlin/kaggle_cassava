{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nbformat as nbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-14 14:28:34,122 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...\n"
     ]
    }
   ],
   "source": [
    "parameters = context.catalog.load('parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_notebook(cells, name):\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cells = [(nbf.v4.new_code_cell(c) if isinstance(c, str) else c) for c in cells]\n",
    "    nb['cells'] = cells\n",
    "    nbf.write(nb, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imports(text):\n",
    "    imports = []\n",
    "    for line in text.split('\\n'):\n",
    "        if (line.startswith('import') \n",
    "            or line.startswith('from') \n",
    "            and (not 'from .' in line)\n",
    "            and not ('kedro' in line)\n",
    "            and not 'cassava' in line):\n",
    "            imports.append(line.strip())\n",
    "    return imports\n",
    "\n",
    "def get_imports():\n",
    "    imports = []\n",
    "    path = '../src/cassava/pipelines'\n",
    "    for dirname in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, dirname)) or dirname.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(os.path.join(path, dirname)):\n",
    "            if not fname.endswith('.py'):\n",
    "                continue\n",
    "            fpath = os.path.join(path, dirname, fname)\n",
    "            with open(fpath) as f:\n",
    "                file_imports = extract_imports(f.read())\n",
    "                imports += file_imports\n",
    "    imports = list(set(imports))\n",
    "    return '\\n'.join(imports)\n",
    "            \n",
    "            \n",
    "def get_helper_files_cells(target_files):\n",
    "    cells = []\n",
    "    \n",
    "    path = '../src/cassava/'\n",
    "    \n",
    "    for fname in target_files:\n",
    "        fpath = os.path.join(path, fname)\n",
    "        \n",
    "        with open(fpath) as f:\n",
    "            file_lines = [line for line in f if not line.startswith('from cassava')]\n",
    "            file_text = ''.join(file_lines)\n",
    "            cells.append(nbf.v4.new_code_cell(f'# file {fname}\\n\\n{file_text}'))\n",
    "    return cells\n",
    "\n",
    "    \n",
    "def get_pipeline_def_cell(pipeline, name):\n",
    "    funcs = [\n",
    "        f\"#Pipeline {name}\"\n",
    "    ]\n",
    "    for node in pipeline.nodes:\n",
    "        func_source_node = inspect.getsource(node._func)\n",
    "        funcs.append(func_source_node)\n",
    "        \n",
    "    cell_code = \"\\n\\n\".join(funcs)\n",
    "    return nbf.v4.new_code_cell(cell_code)\n",
    "\n",
    "def get_pipeline_execution_cells(pipeline):\n",
    "    cells = []\n",
    "    nodesets = pipeline._topo_sorted_nodes\n",
    "    for nodeset in nodesets:\n",
    "        while nodeset:\n",
    "            node = nodeset.pop()\n",
    "            if node.outputs:\n",
    "                node_code = f\"\"\"{\", \".join(node.outputs)} = {node._func_name}({\", \".join(node.inputs)})\"\"\"\n",
    "            else:\n",
    "                node_code = f\"\"\"{node._func_name}({\", \".join(node.inputs)})\"\"\"\n",
    "            \n",
    "            cells.append(nbf.v4.new_code_cell(node_code))\n",
    "    return cells\n",
    "        \n",
    "    \n",
    "def get_notebook_cells(parameters, \n",
    "                       pipelines,\n",
    "                       initial_cells, \n",
    "                       extra_cells, \n",
    "                       final_cells,\n",
    "                       file_list):\n",
    "    imports_cell = nbf.v4.new_code_cell(get_imports())\n",
    "    \n",
    "    helper_files_cells = get_helper_files_cells(file_list)\n",
    "    parameters_cell = nbf.v4.new_code_cell(f\"\"\"parameters = {json.dumps(parameters, indent=4, default=str)}\"\"\")\n",
    "    \n",
    "    pipeline_def_cells = []\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name.startswith('_'):\n",
    "            continue\n",
    "        pipeline_def_cells.append(get_pipeline_def_cell(pipeline, name))\n",
    "    \n",
    "    pipeline_exec_cells = []\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if not name == '__submit__':\n",
    "            continue\n",
    "        pipeline_exec_cells += get_pipeline_execution_cells(pipeline)\n",
    "    return [imports_cell,\n",
    "            *initial_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Functions\"),\n",
    "            *helper_files_cells,\n",
    "            *pipeline_def_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Parameters\"),\n",
    "            parameters_cell, \n",
    "            *extra_cells,\n",
    "            nbf.v4.new_markdown_cell(\"# Execution\"),\n",
    "           *pipeline_exec_cells,\n",
    "           *final_cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cell = \"\"\"\n",
    "!ls /kaggle/input/timm-pretrained-efficientnet\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/timm-pretrained-efficientnet/efficientnet/efficientnet_b0_ra-3dd342df.pth /root/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth\n",
    "\"\"\"\n",
    "\n",
    "installs_cell = \"\"\"\n",
    "!pip install /kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl\n",
    "!pip install /kaggle/input/lmdb-python-package/lmdb-1.0.0/dist/lmdb-1.0.0.tar\n",
    "\"\"\"\n",
    "\n",
    "initial_cells = ['%matplotlib inline',\n",
    "\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\"\"\",\n",
    "                                      models_cell, \n",
    "                                      installs_cell]\n",
    "\n",
    "data_cell = nbf.v4.new_code_cell(\"\"\"\n",
    "DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'\n",
    "\n",
    "train_labels = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "sample_submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "label_num_to_disease_map = pd.read_csv(f'{DATA_DIR}/label_num_to_disease_map.json')\n",
    "\n",
    "train_images_torch = CassavaDataset(image_ids=train_labels.image_id.values, labels=train_labels.label.values, root=f'{DATA_DIR}/train_images')\n",
    "test_images_torch = CassavaDataset(image_ids=sample_submission.image_id.values, labels=sample_submission.label.values, root=f'{DATA_DIR}/test_images')\n",
    "\n",
    "train_images_lmdb = ImageLMDBDataset(db_path='data/03_primary/train.lmdb')\n",
    "test_images_lmdb = ImageLMDBDataset(db_path='data/03_primary/test.lmdb')\n",
    "\n",
    "pretrained_model_path = '/kaggle/input/byol-pretrained-cassava/pretrained_model_best.pt'\n",
    "pretrained_model = LeafDoctorModel(hparams=Namespace(**parameters['classifier']))\n",
    "pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "final_cells = [nbf.v4.new_code_cell(\"print(val_scores)\"),\n",
    "               nbf.v4.new_code_cell(\"submission.to_csv('submission.csv', index=False)\"),\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['transforms.py', 'utils.py', 'models/model.py', 'models/byol.py', 'node_helpers.py', 'lmdb_dataset.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_notebook(\n",
    "    get_notebook_cells(parameters, \n",
    "                       context.pipelines, \n",
    "                       initial_cells=initial_cells,\n",
    "                       extra_cells=[data_cell], \n",
    "                       final_cells=final_cells,\n",
    "                       file_list=file_list), \n",
    "    'submission.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cassava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
