{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from PIL import Image\n",
    "from copy import copy\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import ConcatDataset, Subset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import imagehash\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/kaggle/input/timm-pretrained-efficientnet': No such file or directory\n",
      "mkdir: cannot create directory ‘/root’: Permission denied\n",
      "cp: failed to access '/root/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth': Permission denied\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls /kaggle/input/timm-pretrained-efficientnet\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/timm-pretrained-efficientnet/efficientnet/efficientnet_b0_ra-3dd342df.pth /root/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement '/kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Processing /kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/boris/Documents/kaggle cassava/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install /kaggle/input/timm-package/timm-0.1.26-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file transforms.py\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def data_preapre_transform(image):\n",
    "    \"\"\"Transforms a PIL Image to have aspec ratio 8/6\"\"\"\n",
    "    image = Image.fromarray(np.array(image))\n",
    "    if image.size[0] < image.size[1]:\n",
    "        image = image.rotate(90, expand=True)\n",
    "\n",
    "    # Center crop until 8:6\n",
    "    width, height = image.size   # Get dimensions\n",
    "\n",
    "    if round(width/height, 3) != round(8/6, 3):\n",
    "        new_height = int(height*(width/height * 6/8))\n",
    "        new_width = width\n",
    "\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_wrapper(transforms):\n",
    "    def wraps(img):\n",
    "        return transforms(image=np.array(img))['image']\n",
    "    return wraps\n",
    "\n",
    "\n",
    "def get_byol_transforms(width, height):\n",
    "    byol_transforms = A.Compose([\n",
    "        A.Resize(width, height),\n",
    "        A.ToFloat(max_value=1.0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return get_wrapper(byol_transforms)\n",
    "\n",
    "\n",
    "def get_prepare_transforms(width, height):\n",
    "    prepare_transforms = A.Compose([\n",
    "        A.Resize(width, height),\n",
    "    ])\n",
    "\n",
    "    return get_wrapper(prepare_transforms)\n",
    "\n",
    "\n",
    "def get_train_transforms(width, height):\n",
    "    train_transforms = A.Compose([\n",
    "        A.JpegCompression(quality_lower=95, quality_upper=100, p=0.5),\n",
    "        A.ColorJitter(p=0.5),\n",
    "        A.ToFloat(max_value=1.0),\n",
    "        A.ShiftScaleRotate(p=0.5),\n",
    "        A.RandomResizedCrop(width, height, scale=(0.1, 0.8)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        A.CoarseDropout(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return get_wrapper(train_transforms)\n",
    "\n",
    "\n",
    "def get_test_transforms(width, height):\n",
    "    test_transforms = A.Compose([\n",
    "        A.ToFloat(max_value=1.0),\n",
    "        A.CenterCrop(width, height),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return get_wrapper(test_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file utils.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "\n",
    "class Unnormalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf, annot=box_labels, fmt=\"\", cmap=cmap, cbar=cbar, xticklabels=categories, yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "def plot_image(img, label=None, ax=None):\n",
    "    new_img = torch.Tensor(np.array(img))\n",
    "    label_num_to_disease_map = {0: 'Cassava Bacterial Blight (CBB)',\n",
    "                                1: 'Cassava Brown Streak Disease (CBSD)',\n",
    "                                2: 'Cassava Green Mottle (CGM)',\n",
    "                                3: 'Cassava Mosaic Disease (CMD)',\n",
    "                                4: 'Healthy'}\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "    ax.imshow(new_img.permute(2, 1, 0))\n",
    "    ax.axis('off')\n",
    "    if label is not None:\n",
    "\n",
    "        if isinstance(label, int):\n",
    "            label = label_num_to_disease_map.get(label, 0)\n",
    "        ax.set_title(f'{label}')\n",
    "\n",
    "\n",
    "def plot_label_examples(dataset, targets, target_label):\n",
    "    label_indices = np.where(targets == target_label)[0]\n",
    "\n",
    "    sample = np.random.choice(label_indices, 6)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(2, 3),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.1,  # pad between axes in inch.\n",
    "                     )\n",
    "\n",
    "    for ax, idx in zip(grid, sample):\n",
    "        img, label = dataset[idx]\n",
    "        assert label == target_label\n",
    "        plot_image(img, ax=ax)\n",
    "    plt.suptitle(f'Label {target_label}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None, target_transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        return x, y\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.subset.dataset.labels[self.subset.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, root, image_ids, labels, sources=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.image_ids = image_ids\n",
    "        self.labels = labels\n",
    "        self.targets = self.labels\n",
    "        self.sources = sources\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = io.imread(os.path.join(self.root, self.image_ids[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file models/model.py\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from torch import nn\n",
    "import timm\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class LeafDoctorModel(pl.LightningModule):\n",
    "    def __init__(self, hparams = None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams or Namespace()\n",
    "\n",
    "        self.trunk = timm.create_model('efficientnet_b0', pretrained=True, num_classes=5)\n",
    "\n",
    "        # for layer in [self.trunk.bn1, self.trunk.bn2]:\n",
    "        #     for param in layer.parameters():\n",
    "        #         param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        probabilities = nn.functional.softmax(self.forward(x), dim=1)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, x):\n",
    "        return torch.max(self.forward(x), 1)[1]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                      lr=self.hparams.lr or self.hparams.learning_rate,\n",
    "                                      weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                           max_lr=self.hparams.lr,\n",
    "                                                           epochs=self.hparams.max_epochs,\n",
    "                                                           steps_per_epoch=int(23712/self.hparams.batch_size))\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [\n",
    "                {\n",
    "                    'scheduler': lr_scheduler,\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1,\n",
    "                    'reduce_on_plateau': False,\n",
    "                    'monitor': 'val_loss',\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = bi_tempered_logistic_loss(y_hat, y,\n",
    "                                         self.hparams.bitempered_t1,\n",
    "                                         self.hparams.bitempered_t2,\n",
    "                                         label_smoothing=self.hparams.label_smoothing)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, logger=True),\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file models/byol.py\n",
    "\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from typing import Dict, List\n",
    "import pytorch_lightning as pl\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import optim\n",
    "import torch.nn.functional as f\n",
    "import random\n",
    "from typing import Callable, Tuple, Union\n",
    "from kornia import augmentation as aug\n",
    "from kornia import filters\n",
    "from kornia.geometry import transform as tf\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "def normalized_mse(x: Tensor, y: Tensor) -> Tensor:\n",
    "    x = f.normalize(x, dim=-1)\n",
    "    y = f.normalize(y, dim=-1)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn: Callable, p: float):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x if random.random() > self.p else self.fn(x)\n",
    "\n",
    "\n",
    "def default_aug(image_size: Tuple[int, int] = (360, 360)) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        aug.ColorJitter(contrast=0.1, brightness=0.1, saturation=0.1, p=0.8),\n",
    "        aug.RandomVerticalFlip(),\n",
    "        aug.RandomHorizontalFlip(),\n",
    "        RandomApply(filters.GaussianBlur2d((3, 3), (0.5, 0.5)), p=0.1),\n",
    "        aug.RandomResizedCrop(size=image_size, scale=(0.5, 1)),\n",
    "        aug.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def mlp(dim: int, projection_size: int = 256, hidden_size: int = 4096) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, hidden_size),\n",
    "        nn.BatchNorm1d(hidden_size),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(hidden_size, projection_size),\n",
    "    )\n",
    "\n",
    "\n",
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        projection_size: int = 256,\n",
    "        hidden_size: int = 4096,\n",
    "        layer: Union[str, int] = -2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.projection_size = projection_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer = layer\n",
    "\n",
    "        self._projector = None\n",
    "        self._projector_dim = None\n",
    "        self._encoded = torch.empty(0)\n",
    "        self._register_hook()\n",
    "\n",
    "    @property\n",
    "    def projector(self):\n",
    "        if self._projector is None:\n",
    "            self._projector = mlp(\n",
    "                self._projector_dim, self.projection_size, self.hidden_size\n",
    "            )\n",
    "        return self._projector\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        output = output.flatten(start_dim=1)\n",
    "        if self._projector_dim is None:\n",
    "            self._projector_dim = output.shape[-1]\n",
    "        self._encoded = self.projector(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        if isinstance(self.layer, str):\n",
    "            layer = dict([*self.model.named_modules()])[self.layer]\n",
    "        else:\n",
    "            layer = list(self.model.children())[self.layer]\n",
    "\n",
    "        layer.register_forward_hook(self._hook)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        _ = self.model(x)\n",
    "        return self._encoded\n",
    "\n",
    "\n",
    "class BYOL(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        image_size: Tuple[int, int] = (360, 360),\n",
    "        hidden_layer: Union[str, int] = -2,\n",
    "        projection_size: int = 256,\n",
    "        hidden_size: int = 4096,\n",
    "        augment_fn: Callable = None,\n",
    "        beta: float = 0.99,\n",
    "        hparams = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._augment = default_aug(image_size) if augment_fn is None else augment_fn\n",
    "        self.beta = beta\n",
    "        self.encoder = EncoderWrapper(\n",
    "            model, projection_size, hidden_size, layer=hidden_layer\n",
    "        )\n",
    "        self.predictor = nn.Linear(projection_size, projection_size, hidden_size)\n",
    "        self.hparams = hparams or Namespace()\n",
    "        self._target = None\n",
    "\n",
    "        self.encoder(torch.zeros(2, 3, *image_size))\n",
    "\n",
    "    def augment(self, batch):\n",
    "        if self.hparams.precision == 16:\n",
    "            return self._augment(batch.double()).to(torch.float16)\n",
    "        return self._augment(batch)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.predictor(self.encoder(x))\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        if self._target is None:\n",
    "            self._target = deepcopy(self.encoder)\n",
    "        return self._target\n",
    "\n",
    "    def update_target(self):\n",
    "        for p, pt in zip(self.encoder.parameters(), self.target.parameters()):\n",
    "            pt.data = self.beta * pt.data + (1 - self.beta) * p.data\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                           max_lr=self.hparams.lr,\n",
    "                                                           epochs=self.hparams.max_epochs,\n",
    "                                                           steps_per_epoch=self.hparams.limit_train_batches)\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [\n",
    "                {\n",
    "                    'scheduler': lr_scheduler,\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1,\n",
    "                    'reduce_on_plateau': False,\n",
    "                    'monitor': 'val_loss',\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n",
    "        x = batch[0]\n",
    "        with torch.no_grad():\n",
    "            x1, x2 = self.augment(x), self.augment(x)\n",
    "\n",
    "        pred1, pred2 = self.forward(x1), self.forward(x2)\n",
    "        with torch.no_grad():\n",
    "            targ1, targ2 = self.target(x1), self.target(x2)\n",
    "        loss = torch.mean(normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1))\n",
    "\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n",
    "        x = batch[0]\n",
    "        x1, x2 = self.augment(x), self.augment(x)\n",
    "        pred1, pred2 = self.forward(x1), self.forward(x2)\n",
    "        targ1, targ2 = self.target(x1), self.target(x2)\n",
    "        loss = torch.mean(normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_epoch_end(self, outputs: List[Dict]) -> Dict:\n",
    "        val_loss = sum(x[\"loss\"] for x in outputs) / len(outputs)\n",
    "        self.log(\"val_loss\", val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file node_helpers.py\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def score(predictions, labels):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(predictions, labels),\n",
    "        'f1_score': f1_score(predictions, labels, average='weighted'),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict(model, dataset, indices, batch_size=10, num_workers=4, transform=None):\n",
    "    dataset = DatasetFromSubset(\n",
    "        Subset(dataset, indices=indices),\n",
    "        transform=transform)\n",
    "\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n",
    "\n",
    "    predictions = []\n",
    "    probas = []\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "            batch_probas = model.predict_proba(images)\n",
    "            batch_preds = torch.max(batch_probas, 1)[1]\n",
    "            predictions.append(batch_preds)\n",
    "            probas.append(batch_probas)\n",
    "\n",
    "    predictions = torch.hstack(predictions).flatten().tolist()\n",
    "    probas = torch.vstack(probas).tolist()\n",
    "\n",
    "    return predictions, probas\n",
    "\n",
    "\n",
    "def lr_find(trainer, model, train_data_loader, val_data_loader=None, plot=False):\n",
    "    val_dataloaders = [val_data_loader] if val_data_loader else None\n",
    "\n",
    "    lr_finder = trainer.tuner.lr_find(model,\n",
    "                                      train_dataloader=train_data_loader,\n",
    "                                      val_dataloaders=val_dataloaders)\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title('LR finder results')\n",
    "        lr_finder.plot(suggest=True)\n",
    "        plt.show()\n",
    "\n",
    "    newlr = lr_finder.suggestion()\n",
    "    logging.info('LR finder suggestion: %f', newlr)\n",
    "\n",
    "    return newlr\n",
    "\n",
    "\n",
    "def train_classifier(model, train_loader, hparams):\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"classifier\")\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    trainer = Trainer.from_argparse_args(\n",
    "        hparams,\n",
    "        reload_dataloaders_every_epoch=True,\n",
    "        terminate_on_nan=True,\n",
    "        precision=hparams.precision,\n",
    "        amp_level=hparams.amp_level,\n",
    "        callbacks=[lr_monitor],\n",
    "        log_every_n_steps=hparams.log_every_n_steps,\n",
    "        flush_logs_every_n_steps=hparams.flush_logs_every_n_steps,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    new_model = LeafDoctorModel(hparams)\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    model = new_model\n",
    "\n",
    "    # Training\n",
    "    trainer.fit(model, train_loader)\n",
    "    logging.info('Training finished')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_byol(model, hparams, loader):\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"byol\")\n",
    "    byol = BYOL(model, hparams=hparams)\n",
    "\n",
    "    early_stopping = EarlyStopping('train_loss',\n",
    "                                   mode='min',\n",
    "                                   patience=hparams.early_stop_patience,\n",
    "                                   verbose=True)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "    trainer = Trainer.from_argparse_args(\n",
    "        hparams,\n",
    "        reload_dataloaders_every_epoch=True,\n",
    "        terminate_on_nan=True,\n",
    "        callbacks=[early_stopping, lr_monitor],\n",
    "        precision=hparams.precision,\n",
    "        amp_level=hparams.amp_level,\n",
    "        log_every_n_steps=hparams.log_every_n_steps,\n",
    "        flush_logs_every_n_steps=hparams.flush_logs_every_n_steps,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    if hparams.auto_lr_find:\n",
    "        new_lr = lr_find(trainer, byol, loader)\n",
    "        hparams.lr = new_lr\n",
    "        byol.hparams.lr = new_lr\n",
    "\n",
    "    trainer.fit(byol, loader, loader)\n",
    "    return byol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline prepare\n",
    "\n",
    "def obtain_image_hashes(train_images_torch_2020, train_images_torch_2019, test_images_torch_2019, extra_images_torch_2019):\n",
    "    # Adapted from https://www.kaggle.com/zzy990106/duplicate-images-in-two-competitions\n",
    "    datasets = {\n",
    "        'train_2020': train_images_torch_2020,\n",
    "        'train_2019': train_images_torch_2019,\n",
    "        'test_2019': test_images_torch_2019,\n",
    "        'extra_2019': extra_images_torch_2019,\n",
    "    }\n",
    "\n",
    "    image_ids = []\n",
    "    hashes = []\n",
    "\n",
    "    logging.info('Obtaining hashes')\n",
    "\n",
    "    for dname, ds in tqdm(datasets.items()):\n",
    "        loader = DataLoader(ds, num_workers=6, batch_size=None)\n",
    "        for ix, (image, label) in tqdm(enumerate(loader), total=len(loader), desc=dname):\n",
    "            if dname in ['test_2019', 'extra_2019']:\n",
    "                label = None\n",
    "\n",
    "            if label is not None:\n",
    "                label = int(label)\n",
    "            img_id = (dname, ix, label)\n",
    "            pil_img = Image.fromarray(np.array(image))\n",
    "            hash = get_img_hash(pil_img)\n",
    "            image_ids.append(img_id)\n",
    "            hashes.append(hash)\n",
    "\n",
    "    image_ids_df = pd.DataFrame(image_ids, columns=['ds', 'ix', 'label'])\n",
    "    hashes_df = pd.DataFrame(np.array(hashes).astype(int))\n",
    "\n",
    "    return image_ids_df, hashes_df\n",
    "\n",
    "\n",
    "def find_duplicates(image_ids, image_hashes):\n",
    "    # Adapted from https://www.kaggle.com/zzy990106/duplicate-images-in-two-competitions\n",
    "    image_ids = image_ids.values\n",
    "    hashes = image_hashes.values\n",
    "\n",
    "    hashes_all = np.array(hashes)\n",
    "    hashes_all = torch.Tensor(hashes_all.astype(int))\n",
    "\n",
    "    logging.info('Computing similarities and finding duplicates')\n",
    "    sim_threshold = int(0.9 * hashes_all.shape[1])\n",
    "    duplicates = []\n",
    "    for i in tqdm(range(hashes_all.shape[0])):\n",
    "        sim = ((hashes_all[i] == hashes_all).sum(dim=1).numpy() > sim_threshold).astype(int)\n",
    "        dupes = np.nonzero(sim)[0]\n",
    "        if len(dupes) > 1:\n",
    "            for dup in dupes:\n",
    "                if dup != i:\n",
    "                    duplicates.append(tuple(sorted([i, dup])))\n",
    "\n",
    "    duplicates = list(set(duplicates))\n",
    "\n",
    "    out_rows = []\n",
    "    for duplicate_pair in duplicates:\n",
    "        image_id1 = image_ids[duplicate_pair[0]]\n",
    "        image_id2 = image_ids[duplicate_pair[1]]\n",
    "        out_rows.append(\n",
    "            # ds1 | id1 | label1 | ds2 | id2 | label2\n",
    "            (*image_id1, *image_id2)\n",
    "        )\n",
    "\n",
    "    out_rows = pd.DataFrame(list(set(out_rows)), columns=['ds1', 'id1', 'label1', 'ds2', 'id2', 'label2'])\n",
    "    return out_rows\n",
    "\n",
    "\n",
    "def prepare_dataset(train_images_torch_2020, train_images_torch_2019, test_images_torch_2019, extra_images_torch_2019, duplicates):\n",
    "    blacklist = dict(duplicates[['ds2', 'id2']].groupby('ds2').agg({'id2': list})['id2'])\n",
    "\n",
    "    train_images_torch_2020.transform = data_preapre_transform\n",
    "    train_images_torch_2019.transform = data_preapre_transform\n",
    "    test_images_torch_2019.transform = data_preapre_transform\n",
    "    extra_images_torch_2019.transform = data_preapre_transform\n",
    "\n",
    "    prepare_transforms = get_prepare_transforms(512, 512)\n",
    "    train_dataset_2020 = DatasetFromSubset(\n",
    "        Subset(train_images_torch_2020, indices=[i for i in range(len(train_images_torch_2020)) if i not in blacklist['train_2020']]),\n",
    "        transform=prepare_transforms)\n",
    "\n",
    "    train_dataset_2019 = DatasetFromSubset(\n",
    "        Subset(train_images_torch_2019,\n",
    "               indices=[i for i in range(len(train_images_torch_2019)) if i not in blacklist['train_2019']]),\n",
    "        transform=prepare_transforms)\n",
    "\n",
    "    test_dataset_2019 = DatasetFromSubset(\n",
    "        Subset(test_images_torch_2019,\n",
    "               indices=[i for i in range(len(test_images_torch_2019)) if i not in blacklist['test_2019']]),\n",
    "        transform=prepare_transforms, target_transform=lambda y: -1)\n",
    "\n",
    "    extra_images_torch_2019 = DatasetFromSubset(\n",
    "        Subset(extra_images_torch_2019,\n",
    "               indices=[i for i in range(len(extra_images_torch_2019)) if i not in blacklist['extra_2019']]),\n",
    "        transform=prepare_transforms, target_transform=lambda y: -1)\n",
    "\n",
    "    train_dataset = ConcatDataset([train_dataset_2020, train_dataset_2019])\n",
    "    train_sources = ['train_2020']*len(train_dataset_2020) + ['train_2019']*len(train_dataset_2019)\n",
    "\n",
    "    unlabelled_dataset = ConcatDataset([test_dataset_2019, extra_images_torch_2019])\n",
    "    unlabelled_sources = ['test_2019'] * len(test_dataset_2019) + ['extra_2019'] * len(extra_images_torch_2019)\n",
    "\n",
    "    train_path = 'data/03_primary/train'\n",
    "    train_csv_path = 'data/03_primary/train.csv'\n",
    "    unlabelled_path = 'data/03_primary/unlabelled'\n",
    "    unlabelled_csv_path = 'data/03_primary/unlabelled.csv'\n",
    "\n",
    "    if any([os.path.exists(train_path),\n",
    "            os.path.exists(unlabelled_path)]):\n",
    "        raise Exception('Dataset folders already exist, delete manually to overwrite.')\n",
    "\n",
    "    os.makedirs(train_path, exist_ok=True)\n",
    "    os.makedirs(unlabelled_path, exist_ok=True)\n",
    "\n",
    "    def make_image_folder(dataset, sources, path, csv_path):\n",
    "        loader = DataLoader(dataset, batch_size=None, num_workers=6, collate_fn=lambda x: x)\n",
    "        rows = []\n",
    "        for ix, (image, label) in enumerate(tqdm(loader)):\n",
    "            image_id = f'{ix}.jpg'\n",
    "            source = sources[ix]\n",
    "            img_path = os.path.join(path, image_id)\n",
    "            imsave(img_path, image)\n",
    "            rows.append((image_id, label, source))\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=['image_id', 'label', 'source'])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return df\n",
    "\n",
    "    train_df = make_image_folder(train_dataset, train_sources, train_path, train_csv_path)\n",
    "    unlabelled_df = make_image_folder(unlabelled_dataset, unlabelled_sources, unlabelled_path, unlabelled_csv_path)\n",
    "    return CassavaDataset(train_path, train_df.image_id, train_df.label), CassavaDataset(unlabelled_path, unlabelled_df.image_id, unlabelled_df.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline pretrain\n",
    "\n",
    "def pretrain_model(train, unlabelled, parameters):\n",
    "    byol_transforms = get_byol_transforms(parameters['byol']['width'], parameters['byol']['height'])\n",
    "    train_dataset = DatasetFromSubset(\n",
    "        Subset(train, indices=list(range(len(train)))),\n",
    "        transform=byol_transforms)\n",
    "    unlabelled_dataset = DatasetFromSubset(\n",
    "        Subset(unlabelled, indices=list(range(len(unlabelled)))),\n",
    "        transform=byol_transforms)\n",
    "    dataset = ConcatDataset([train_dataset, unlabelled_dataset])\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=parameters['byol']['batch_size'],\n",
    "                        num_workers=parameters['data_loader_workers'],\n",
    "                        shuffle=True)\n",
    "\n",
    "    classifier_params = Namespace(**parameters['classifier'])\n",
    "    model = LeafDoctorModel(classifier_params)\n",
    "\n",
    "    hparams = Namespace(**parameters['byol'])\n",
    "    byol = train_byol(model.trunk, hparams, loader)\n",
    "\n",
    "    state_dict = byol.encoder.model.state_dict()\n",
    "    model = LeafDoctorModel(classifier_params)\n",
    "    model.trunk.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline train\n",
    "\n",
    "def train_model(finetuned_model, train, parameters):\n",
    "    train_transform = get_train_transforms(parameters['classifier']['train_width'], parameters['classifier']['train_height'])\n",
    "\n",
    "    train_dataset = DatasetFromSubset(Subset(train, indices=list(range(len(train)))),\n",
    "                                      transform=train_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                                batch_size=parameters['classifier']['batch_size'],\n",
    "                                num_workers=parameters['data_loader_workers'],\n",
    "                                shuffle=True)\n",
    "\n",
    "    hparams = Namespace(**parameters['classifier'])\n",
    "\n",
    "    # Train\n",
    "    logging.info('Training model')\n",
    "    hparams.max_epochs = hparams.max_epochs - hparams.finetune_epochs\n",
    "    model = train_classifier(finetuned_model, train_loader, hparams=hparams)\n",
    "\n",
    "    # Finetune for test image size\n",
    "    hparams = Namespace(**parameters['classifier'])\n",
    "    logging.info('Finetuning model for test image size')\n",
    "    train_transform = get_train_transforms(parameters['classifier']['test_width'],\n",
    "                                           parameters['classifier']['test_height'])\n",
    "    train_dataset = DatasetFromSubset(Subset(train, indices=list(range(len(train)))),\n",
    "                                      transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=parameters['classifier']['batch_size'],\n",
    "                              num_workers=parameters['data_loader_workers'],\n",
    "                              shuffle=True)\n",
    "    hparams.max_epochs = hparams.finetune_epochs\n",
    "    hparams.lr = hparams.lr/10\n",
    "    model = train_classifier(model, train_loader, hparams=hparams)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline predict\n",
    "\n",
    "def predict_submission(cv_results, test_images_torch_2020, sample_submission, parameters):\n",
    "    logging.debug('Predicting on test with model')\n",
    "\n",
    "    fold_model_names = [cv_results[fold]['model_path'] for fold in cv_results if fold != 'summary']\n",
    "\n",
    "    all_probas = []\n",
    "    for model_path in fold_model_names:\n",
    "        model = LeafDoctorModel()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        predictions, probas = predict(model,\n",
    "                                  dataset=test_images_torch_2020,\n",
    "                                  indices=list(range(len(test_images_torch_2020))),\n",
    "                                  batch_size=parameters['eval']['batch_size'],\n",
    "                                  num_workers=parameters['data_loader_workers'],\n",
    "                                  transform=get_test_transforms(parameters['classifier']['test_width'], parameters['classifier']['test_height']))\n",
    "\n",
    "        all_probas.append(probas)\n",
    "\n",
    "    aggregated_probas = np.mean(all_probas, axis=0).reshape(-1, 5)\n",
    "    pred_labels = np.argmax(aggregated_probas, 1)\n",
    "    sample_submission.label = pred_labels\n",
    "    return sample_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline cv\n",
    "\n",
    "def obtain_cv_splits(train, parameters):\n",
    "    labels = train.labels\n",
    "    sources = train.sources\n",
    "    indices_2020 = np.argwhere(sources == 'train_2020').flatten()\n",
    "    indices_2019 = np.argwhere(sources == 'train_2019').flatten()\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=parameters['cv_splits'], random_state=parameters['seed'])\n",
    "\n",
    "    splits = []\n",
    "    # Preserve same class distribution in both train and test\n",
    "    # Only put 2020 data in test\n",
    "    splits_2019 = list(cv.split(indices_2019, labels[indices_2020][:len(indices_2019)]))\n",
    "    splits_2020 = list(cv.split(indices_2020, labels[indices_2020]))\n",
    "    for (train_2019_idx, test_2019_idx), (train_2020_idx, test_2020_idx) in zip(splits_2019, splits_2020):\n",
    "        train_idx = np.concatenate([indices_2019[train_2019_idx], indices_2020[train_2020_idx]])\n",
    "        test_idx = indices_2020[test_2020_idx]\n",
    "        splits.append((train_idx, test_idx))\n",
    "    return splits\n",
    "\n",
    "\n",
    "def cross_validation(train, unlabelled, cv_splits, parameters):\n",
    "    cv_results = {\n",
    "        'summary': {},\n",
    "    }\n",
    "    score_values = {\n",
    "        'test': defaultdict(list),\n",
    "        'val': defaultdict(list),\n",
    "    }\n",
    "\n",
    "    if os.path.exists(parameters['cv_models_dir']) and len(os.listdir(parameters['cv_models_dir'])) > 0:\n",
    "        raise Exception('CV models path already exists, please delete it explicitly to overwrite')\n",
    "    else:\n",
    "        os.makedirs(parameters['cv_models_dir'], exist_ok=True)\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        logging.info('Fitting CV fold %d', fold_num)\n",
    "        model_path = os.path.join(parameters['cv_models_dir'], f'model_fold_{fold_num}.pt')\n",
    "        fold_parameters = copy(parameters)\n",
    "\n",
    "        fold_train_dataset = DatasetFromSubset(Subset(train, indices=train_idx))\n",
    "        fold_test_dataset = DatasetFromSubset(Subset(train, indices=test_idx))\n",
    "\n",
    "        # Split\n",
    "        logging.info('Pretraining on train+unlabelled')\n",
    "        pretrained_model = pretrain_model(fold_train_dataset, unlabelled, fold_parameters)\n",
    "\n",
    "        logging.info('Training on train')\n",
    "        model = train_model(pretrained_model, fold_train_dataset, fold_parameters)\n",
    "\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Score on test\n",
    "        test_scores, test_predictions = score_model(model, fold_test_dataset, list(range(len(fold_test_dataset))), fold_parameters)\n",
    "\n",
    "        cv_results[f'fold_{fold_num}'] = {\n",
    "            'model_path': model_path,\n",
    "            'test_indices': test_idx,\n",
    "            'test_scores': test_scores,\n",
    "            'test_predictions': test_predictions,\n",
    "        }\n",
    "\n",
    "        for score in test_scores:\n",
    "            score_values['test'][score].append(test_scores[score])\n",
    "\n",
    "    for score_set in score_values:\n",
    "        for score_name, scores in score_values[score_set].items():\n",
    "            cv_results['summary'][f'{score_set}_{score_name}_mean'] = np.mean(scores)\n",
    "            cv_results['summary'][f'{score_set}_{score_name}_std'] = np.std(scores)\n",
    "\n",
    "    logging.info('Cross-validation results %s', cv_results['summary'])\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline finetune\n",
    "\n",
    "def finetune_on_test(pretrained_model, train_images_lmdb, test_images_torch_2020, parameters):\n",
    "    byol_transforms = get_byol_transforms(parameters['byol']['width'], parameters['byol']['height'])\n",
    "    dataset = torch.utils.data.ConcatDataset([train_images_lmdb, test_images_torch_2020])\n",
    "    dataset = DatasetFromSubset(\n",
    "        torch.utils.data.Subset(dataset, indices=list(range(len(dataset)))),\n",
    "        transform = byol_transforms)\n",
    "    loader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=parameters['byol']['batch_size'],\n",
    "                                         num_workers=parameters['data_loader_workers'],\n",
    "                                         shuffle=True)\n",
    "\n",
    "    byol_params = dict(parameters['byol'])\n",
    "    byol_test_overrides = dict(parameters['byol']['on_test'])\n",
    "    byol_params.update(byol_test_overrides)\n",
    "\n",
    "    hparams = Namespace(**byol_params)\n",
    "\n",
    "    state_dict = pretrained_model.state_dict()\n",
    "    model = LeafDoctorModel(Namespace(**parameters['classifier']))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    byol = train_byol(pretrained_model.trunk, hparams, loader)\n",
    "\n",
    "    state_dict = byol.encoder.model.state_dict()\n",
    "    model = LeafDoctorModel(Namespace(**parameters['classifier']))\n",
    "    model.trunk.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"seed\": 42,\n",
    "    \"cv_splits\": 4,\n",
    "    \"cv_models_dir\": \"data/06_models/cv_folds\",\n",
    "    \"validation_size\": 0.15,\n",
    "    \"data_loader_workers\": 4,\n",
    "    \"classifier\": {\n",
    "        \"train_height\": 320,\n",
    "        \"train_width\": 320,\n",
    "        \"test_height\": 400,\n",
    "        \"test_width\": 400,\n",
    "        \"gpus\": -1,\n",
    "        \"batch_size\": 20,\n",
    "        \"accumulate_grad_batches\": 1,\n",
    "        \"max_epochs\": 20,\n",
    "        \"finetune_epochs\": 5,\n",
    "        \"max_steps\": 0,\n",
    "        \"auto_lr_find\": 0,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"bitempered_t1\": 0.8,\n",
    "        \"bitempered_t2\": 1.2,\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"amp_level\": \"O2\",\n",
    "        \"precision\": 16,\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"flush_logs_every_n_steps\": 100\n",
    "    },\n",
    "    \"byol\": {\n",
    "        \"width\": 400,\n",
    "        \"height\": 400,\n",
    "        \"gpus\": -1,\n",
    "        \"batch_size\": 12,\n",
    "        \"accumulate_grad_batches\": 1,\n",
    "        \"max_epochs\": 100,\n",
    "        \"max_steps\": 0,\n",
    "        \"auto_lr_find\": 0,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"limit_train_batches\": 100,\n",
    "        \"limit_val_batches\": 1,\n",
    "        \"early_stop_patience\": 3,\n",
    "        \"amp_level\": \"02\",\n",
    "        \"precision\": 16,\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"flush_logs_every_n_steps\": 100,\n",
    "        \"on_test\": {\n",
    "            \"lr\": 0.0001,\n",
    "            \"auto_lr_find\": 0,\n",
    "            \"max_epochs\": 5,\n",
    "            \"early_stop_patience\": 1\n",
    "        }\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"batch_size\": 16\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/cassava-leaf-disease-classification/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-71cd9ee78426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODELS_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/cassava-models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabel_num_to_disease_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/label_num_to_disease_map.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle cassava/venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle cassava/venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle cassava/venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle cassava/venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle cassava/venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cassava-leaf-disease-classification/train.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'\n",
    "MODELS_DIR = '/kaggle/input/cassava-models'\n",
    "\n",
    "train_labels = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "sample_submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "label_num_to_disease_map = pd.read_csv(f'{DATA_DIR}/label_num_to_disease_map.json')\n",
    "\n",
    "test_images_torch_2019 = CassavaDataset(image_ids=sample_submission.image_id.values, labels=sample_submission.label.values, root=f'{DATA_DIR}/test_images')\n",
    "\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "\n",
    "cv_results = {\n",
    "    'fold_0': {\n",
    "        'model_path': os.path.join(MODELS_DIR, 'model_fold_0.pt')\n",
    "    },\n",
    "    'fold_1': {\n",
    "        'model_path': os.path.join(MODELS_DIR, 'model_fold_1.pt')\n",
    "    },\n",
    "    'fold_2': {\n",
    "        'model_path': os.path.join(MODELS_DIR, 'model_fold_2.pt')\n",
    "    },\n",
    "    'fold_3': {\n",
    "        'model_path': os.path.join(MODELS_DIR, 'model_fold_3.pt')\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predict_submission(cv_results, test_images_torch_2020, sample_submission, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cassava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
